{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Driving Car Engineer Nanodegree\n",
    "\n",
    "\n",
    "## Project 5: **Vehicle Detection and Tracking** \n",
    "***\n",
    "### The Goal of this Project\n",
    "In this project, your goal is to write a software pipeline to identify vehicles in a video from a front-facing camera on a car. The test images and project video are available in the [project repository](https://github.com/udacity/CarND-Vehicle-Detection). There is an [writeup template](https://github.com/udacity/CarND-Vehicle-Detection/blob/master/writeup_template.md) in the repository provided as a starting point for your writeup of the project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The steps of this project are the following:\n",
    "\n",
    "#### I - Extract Features\n",
    "* Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images.\n",
    "* Optionally, you can also apply a color transform and append binned color features, as well as histograms of color, to your HOG feature vector. \n",
    "\n",
    "#### II - Build and Train a Classifier\n",
    "* Train a classifier Linear SVM classifier.\n",
    "* Note: for those first two steps don't forget to normalize your features and randomize a selection for training and testing.\n",
    "\n",
    "#### III - Conduct a Sliding Window Search\n",
    "* Implement a sliding-window technique and use your trained classifier to search for vehicles in images.\n",
    "\n",
    "#### IV - Video Implementation\n",
    "* Run your pipeline on a video stream (start with the test_video.mp4 and later implement on full project_video.mp4) and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.\n",
    "* Estimate a bounding box for vehicles detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Rubric](https://review.udacity.com/#!/rubrics/513/view) Points\n",
    "#### Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given Images\n",
    "```\n",
    "[//]: # (Image References)\n",
    "[image1]: ./examples/car_not_car.png\n",
    "[image2]: ./examples/HOG_example.jpg\n",
    "[image3]: ./examples/sliding_windows.jpg\n",
    "[image4]: ./examples/sliding_window.jpg\n",
    "[image5]: ./examples/bboxes_and_heat.png\n",
    "[image6]: ./examples/labels_map.png\n",
    "[image7]: ./examples/output_bboxes.png\n",
    "[video1]: ./project_video.mp4\n",
    "```\n",
    "\n",
    "#![alt text][image2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I - Histogram of Oriented Gradients (HOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Read the training images (vehicle and non-vehicle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in all the vehicles and non-vehicle images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly display one example for vehicle and non-vehicle images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Explore different color spaces and try various combinations of  `skimage.hog()` parameters\n",
    "`orientations`, `pixels_per_cell`, and `cells_per_block`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# color spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hog parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display random images from each of the two classes vehicle and non-vehicle\n",
    "# to get a feel for what the `skimage.hog()` output looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Example using the `YCrCb` color space and HOG parameters \n",
    "`orientations=8`, `pixels_per_cell=(8, 8)` and `cells_per_block=(2, 2)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Final choice of HOG parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Extract HOG features from the training images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II - Build and Train a Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Train a classifier using the selected HOG features (and color features if used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III -Sliding Window Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Implement a sliding window search.  \n",
    "- Decide what scales to search?\n",
    "- How much to overlap windows?\n",
    "- How to optimize the performance of this classifier?\n",
    "\n",
    "(Example: searched on two scales using YCrCb 3-channel HOG features plus spatially binned color and histograms of color in the feature vector, which provided a nice result.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sliding Window Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Examples of test images to demonstrate how the pipeline is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dispay test images to show how the pipeline is working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  IV - Video Implementation\n",
    "\n",
    "#### 1. Test pipeline on the entire project video.\n",
    "(somewhat wobbly or unstable bounding boxes are ok as long as you are identifying the vehicles most of the time with minimal false positives.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Implement some kind of filter for false positives and some method for combining overlapping bounding boxes.\n",
    "- Record the positions of positive detections in each frame of the video. \n",
    "- From the positive detections create a heatmap.\n",
    "- Thresholded that map to identify vehicle positions.\n",
    "- Use `scipy.ndimage.measurements.label()` to identify individual blobs in the heatmap.\n",
    "- Assume each blob corresponded to a vehicle, construct bounding boxes to cover the area of each blob detected.\n",
    "- Display result showing the heatmap from a series of frames of video, the result of `scipy.ndimage.measurements.label()` and the bounding boxes then overlaid on the last frame of video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here are six frames and their corresponding heatmaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is the output of `scipy.ndimage.measurements.label()` on the integrated heatmap from all six frames:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here the resulting bounding boxes are drawn onto the last frame in the series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Here's a [link to my video result](./project_video_result.mp4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "#### Problems / issues aced in implementation of this project.  \n",
    "- What approach and techniques were used?\n",
    "- What worked and why?\n",
    "- Where will the pipeline likely fail?  \n",
    "- What could be further done to improve and make it more robust?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
